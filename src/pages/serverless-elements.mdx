export const title = "Lambdas, queues, etc"

export const description = "Learn how lambdas and queues works in the serverless approach"

export const image = "/queue.jpg"
import { Giphy } from "../components/giphy"

# Elements of serverless ‚Äì¬†lambdas, queues, gateways, and more

We mentioned lambdas, queues, and other elements of serverless in previous chapters ‚Äì [Architecture Principles](/serverless-architecture-principles) and [Serverless Flavors](/serverless-flavors) ‚Äì but what *are* they? 

Let me explain.

## Lambda ‚Äì¬†a cloud function

"Lambda" comes from [lambda calculus](https://en.wikipedia.org/wiki/Lambda_calculus), a mathematical definition of functional programming Alonzo Church introduced in the 1930s. It is the alternative to Turing's [turing machines](https://en.wikipedia.org/wiki/Turing_machine) for describing a system that can [solve any solvable problem](https://en.wikipedia.org/wiki/Church%E2%80%93Turing_thesis). Turing machines describe iterative step-by-step programming, lambda calculus describes functions-calling-functions functional programming.

Both approaches are equal in power.

No wonder that AWS decided to name their cloud functions AWS Lambda. As the platform's popularity grew, the word "lambda" morphed into a generic term for cloud functions and the core building block of serverless computing.

**A lambda is a function.** In this context, a function running as its own tiny server when triggered by an event.

Here's a lambda function (yes just like ATM Machine) that returns `"Hello world"` in response to an HTTP event.

```typescript
// src/handler.ts

import { APIGatewayEvent } from "aws-lambda";

export const handler = async (event: APIGatewayEvent) => {
	return {
		statusCode: 200
		body: "Hello world"
	}
}
```

A TypeScript file exports a function called `handler`. The function accepts an event and returns a response. The AWS Lambda machinery takes care of the rest.

Because this is a user-facing API method, it accepts an AWS API Gateway event and returns an HTTP style response. Status code and body.

Other providers and services will have different events and expect different responses, but **a lambda always follows this pattern üëâ function with an event and a return value.**

### Considerations with lambda functions

Your functions should follow functional programming practices:

- **idempotent** ‚Äì multiple invocations with the same inputs must produce the same result
- **pure** ‚Äì a function can't rely on anything it isn't fetching itself. Your environment does not persist, data in local memory can vanish any time. Rely on the arguments you're given and nothing else.
- **light on side-effects** ‚Äì you need *some* side-effects to make changes to your system. Make sure those come in the form of invoking other functions and services. *State inside your lambda does not persist*
- **do one thing and one thing only** ‚Äì¬†small functions focused on a single task are easiest to understand and combine

Small functions work together to produce extraordinary results. Like this example of [combining Twilio and AWS Lambda to answer the door](https://swizec.com/blog/how-i-answer-the-door-with-aws-lambda-and-twilio/swizec/9255).

### Creating lambdas

In the open source Serverless Framework, you define lambda functions with `serverless.yml` like this:

```yaml
functions:
    helloworld:
        handler: dist/helloworld.handler
        events:
            - http:
                  path: helloworld
                  method: GET
                  cors: true
```

Define a `helloworld` function and say it's handled by the `handler` method exported from the `dist/helloworld` file. Notice that we're running a build step for TypeScript so while the code is in `src/`, we run it from `dist/`.

`events` lists triggers to fire this function. An HTTP GET request on the path `/helloworld` for us.

Other common triggers include Queues, S3 changes, and CloudWatch events. At least in the AWS world.

## Queue

Queue is short for [message queue](https://en.wikipedia.org/wiki/Message_queue) ‚Äì¬†a service built on top of [queue, the data structure](https://en.wikipedia.org/wiki/Queue_(abstract_data_type)). Software engineers aren't that inventive with names and like to overload words ü§∑‚Äç‚ôÇÔ∏è

You can think of the queue data structure as a list of items.

![](../images/queue.png)

`enqueing` adds items to the back of a queue, `dequeing` takes them out the front. Items in the middle wait their turn. Just like a lunch-time burrito queue at a food truck. First come first serve, or FIFO for short (first in first out).

<div id="lock" />

**A messaging queue takes this data structure and scales it into a service.** 

Different implementations of messaging queues exist and they all share these core properties:

- **persistent storage** ‚Äì¬†queues have to be reliable so they store messages in a database. Redis is common for its speed, Postgres works, too. You could even write to a file. Most queues prioritize speed and use in-memory storage.
- **a worker process** ‚Äì¬†once you have messages, you need to process them. Often a process periodically checks, if there's something new on the queue [(polling)](https://en.wikipedia.org/wiki/Polling_(computer_science)). Another approach is triggering this check every time a message arrives.
- **a trigger API** ‚Äì¬†when the queue notices a new message, it triggers your code. In serverless that means running your lambda, in traditional environments it's a worker process of some sort.
- **a retry/error policy** ‚Äì most queue services help you deal with errors. When you fail to process a message, it goes back on the queue to be retried later.

Many modern queues add time to the mix. You can *schedule* messages to be processed in a certain amount of time. Like 5 minutes from now. Or 5 days.

**Time is particularly useful when dealing with errors.**

Server processes can [fail for any reason at any time](/architecture-principles). The error is often temporary so queues use what's called exponential backoff. Often fibonacci backoff.

You try to process a message. It fails. You try again 1 second later. It fails. Retry in 2 seconds. Fail. 4 seconds ...

I've seen corrupt messages go as far as being retried a few days later. We'll talk more about dealing with poison pills in the [Robust Backend Design](/robust-backend-design) chapter.

### Defining a queue

AWS SimpleQueueService is the easiest queue service I've found for use in the AWS serverless ecosystem. There are more powerful alternatives, but they require more setup and have features you often don't need.

> Always use the simplest and smallest service that solves your problem until you know *why* you need something better ‚úåÔ∏è

Using the serverless framework, you define an SQS queue in your `resources` section like this:

```yaml
resources:
    Resources:
        MyQueue:
            Type: "AWS::SQS::Queue"
            Properties:
                QueueName: "MyQueue-${self:provider.stage}"
```

A resource name `MyQueue` of the SQS Queue type with a queue name of `MyQueue-{stage}`. Adding the stage to your queue name allows you to deploy to different logical environments without collisions.

### Processing a queue

You're also going to need a lambda to process messages from this queue.

```yaml
functions:
    myQueueProcess:
        handler: dist/lambdas/myQueue.handler
        events:
            - sqs:
                  arn:
                      Fn::GetAtt:
                          - MyQueue
                          - Arn
                  batchSize: 1
```

A lambda function triggered from messages on your queue. With a `batchSize` of 1, each message is processed on its own lambda ‚Äì¬†a good practice for initial implementations.

We'll talk more about batch sizes in the [Lambda Workflows](/lambda-workflows) and [Robust Backend Design](/robust-backend-design) chapters.

That `myQueue.handler` lambda function would look something like this:

```typescript
import { SQSEvent, SQSRecord } from "aws-lambda";

export const handler = async (event: SQSEvent) => {
    // N depends on batchSize setting
    const messages: string[] = event.Records.map(
        (record: SQSRecord) => record.body
    );

    // do something with the message
    // throw Error() on fail

    return true;
};
```

You have an async function that accepts an `SQSEvent`,¬†as opposed to an `APIGatewayEvent` earlier. This event contains multiple messages depending on `batchSize` and you should never assume the total number. Someone can change that setting.

Extract messages into an array of strings ‚Äì¬†messages are always stringified ‚Äì¬†and process them. Throw an error when something goes wrong so SQS can retry, otherwise return true as a good practice.

> And that's how you process events asynchronously in the serverless world. Great for sending email, data pipelines, processing payments, and anything where you don't want the user waiting around for feedback. ‚úåÔ∏è

## API Gateway

You might not realize it, but most servers aren't actually exposed to the internet.

<Giphy search="whaaat" />

That's right, they're protected by a [reverse proxy](https://en.wikipedia.org/wiki/Reverse_proxy) ‚Äì¬†a server that takes raw requests from the internet and passes them on to the application server.

![](../images/api-gateway.png)

Requests come from the wild internet into the proxy. The proxy then decides what to do:

- is this a valid request?
- will this request break things?
- which server should handle this?

Many requests can be dropped at the proxy level. Distributed denial of service attacks, requests we can't serve, even certain permissions can be verified.

After basic validation, the request is passed on to the correct application server. This is also where we can perform [horizontal scaling](https://en.wikipedia.org/wiki/Scalability#HORIZONTAL-SCALING) and have multiple application servers handling similar requests.

In the serverless world these reverse proxies are called API Gateways and they perform the exact same function: Take requests, do some filtering, pass them on.

Usually triggering a lambda, calling your serverless function. A new mini server spins up, handles the request, and goes away. Near infinite horizontal scaling!

## Static file storage ‚Äì S3

The serverless world is ephemeral so you can't save files just anywhere. Keeping them in your code makes spinning up new containers costly, and you can't save them locally from a request since the server is going away.

Static file storage exists to solve that problem. AWS S3 being the most common example. You may have heard of FTP servers too, if you're old enough üôÉ

You can think of this as a hard drive in the cloud with an API attached. Send a request to save a file, get its URL back.

The provider takes care of serving that URL from a server optimized for static files. No code execution, no edits, no deletes. Just sending a raw file in response to an HTTP request.

You pay for bandwidth and some small fee for storage space. Files you never read often cost fractions of a cent per month.

## Static file server ‚Äì¬†CDN

CDNs ‚Äì¬†[content delivery networks](https://en.wikipedia.org/wiki/Content_delivery_network) ‚Äì¬†present an evolution on static file storage. They work more like distributed caching systems.

Where S3 serves your files from a central location, a CDN tries to serve those same files from as close to the end user as possible. Reducing round-trip times and speeding up your website.

The configuration from your end goes something like this:

1. Point the CDN to your static file URL
2. The CDN gives you a new URL
3. Use *that* URL in your client-side code

You can often automate this with build tools. I believe Netlify and Zeit both do it for you.

Now when a browser requests that file, the URL resolution finds the physically nearest server to your user. Request goes to that server, if it has the file it returns it. Otherwise it goes back to your native URL, loads the file in its own memory, and *then* serves it.

And now your JavaScript, HTML, images, CSS, and others are fast to load anywhere in the world. üëå

## Logging

Logging is one of the hardest problems in a distributed multi-service world. You can't just print to the console or write to a local file.

Nobody's looking at the console and files vanish after every request.

So what do you do?

You build a logging service and send logs to an API that collects them all in a central location. Often these logs are organized by service or lambda function so they're easier to follow.

Implementing a logging service yourself can be tricky (I've done it and don't recommend) and luckily the AWS ecosystem has you covered with CloudWatch. The UI tools for reading those logs aren't the best, but they're a great first start.

Anything you `console.log` on AWS gets collected in CloudWatch. Making this an easy first logging service to use.

# Those are the important bits

Now you know the most important elements of your serverless ecosystem:

- lambdas for doing things
- queues for communicating
- gateways for handling requests
- S3 for static files
- CDN for serving static files
- logging to keep track

There's a bunch more to discover, of course, but that's the core. Next chapter we're gonna look at storing your data.

